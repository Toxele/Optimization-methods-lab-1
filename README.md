# Задача с codeforces:
https://codeforces.com/problemset/problem/2167/G
<img width="1285" height="183" alt="image" src="https://github.com/user-attachments/assets/cbb0541a-cc3c-4a67-9ada-1f1e92d307f4" />
Решение на c++
```
#include <iostream>
#include <vector>
#include <algorithm>
using namespace std;

using ll = long long;
int main() {
    ios::sync_with_stdio(false);
    cin.tie(nullptr);

    int t;
    cin >> t;
    while (t--) 
    {
        int n;
        cin >> n;
        vector<ll> a(n, 0), c(n, 0);
        ll total_cost = 0;

        for (int i = 0; i < n; i++) 
            cin >> a[i];
        for (int i = 0; i < n; i++) 
        {
            cin >> c[i];
            total_cost += c[i];
        }

        vector<ll> dp(c);

        for (int i = 0; i < n; i++) 
        {
            for (int j = 0; j < i; j++) 
            {
                if (a[j] <= a[i]) 
                { 
                    if (dp[j] + c[i] > dp[i]) 
                    {
                        dp[i] = dp[j] + c[i];
                    }
                }
            }
        }

        ll max_keep = *max_element(dp.begin(), dp.end());
        cout << total_cost - max_keep << '\n';
    }

    return 0;
}
```
Идея в том, чтобы не искать лучшие комбинации для минимизации трат на изменения, а максимизировать экономию на тех, кого нам предпочтительнее оставить. Т.е. в dp мы пытаемся хранить сумму тех элементов, которые не хотим менять в силу дороговизны изменения их последовательности. Итоговая стоимость = изначальная стоимость - стоимость экономии

Задачи №1 и №3 с визуализацией будут представлены отдельным Jupyter-ноутбуком

Так же стоит расписать используемую математику и идеи в заданиях №1 и №3, оформить некоторые выводы

# Минималистичный оптимизатор SCALE

Статья:
>https://arxiv.org/abs/2506.16659?spm=a2ty_o01.29997173.0.0.57ec5171z5GqBD&file=2506.16659

Рассматривается задача стохастической оптимизации:

$$
\min_{\theta} \ \mathbb{E}_{\xi \sim \mathcal{D}} \big[ \ell(\theta; \xi) \big],
$$

где $\theta = [\theta^{(1)}, \dots, \theta^{(L)}]$ — параметры модели, разбитые по слоям, а $\ell$ — функция потерь. На шаге $t$ вычисляется стохастический градиент по мини-батчу:

$$
g_t^{(l)} = \nabla_{\theta^{(l)}} \ell(\theta_t; \xi_t).
$$

---

## 1. Столбцовая нормализация градиентов

**Гипотеза авторов**: градиенты весовых матриц могут иметь сильно различающиеся масштабы по разным выходным направлениям (столбцам). Это мешает стабильному обучению. Чтобы устранить зависимость от масштаба, достаточно нормализовать каждый столбец градиента независимо.

**Реализация**: для весовой матрицы $\theta^{(l)} \in \mathbb{R}^{d_{\text{in}} \times d_{\text{out}}}$, её градиент $g_t^{(l)} \in \mathbb{R}^{d_{\text{in}} \times d_{\text{out}}}$ преобразуется как:

$$
\tilde{g}_{t,:,j}^{(l)} = \frac{g_{t,:,j}^{(l)}}{\|g_{t,:,j}^{(l)}\|_2 + \varepsilon}, \quad j = 1, \dots, d_{\text{out}},
$$

где $\varepsilon > 0$ — малая константа для численной стабильности.

Для векторных параметров (например, смещений):

$$
\tilde{g}_t^{(l)} = \frac{g_t^{(l)}}{\|g_t^{(l)}\|_2 + \varepsilon}.
$$

Обновление параметров:

$$
\theta_{t+1}^{(l)} = \theta_t^{(l)} - \eta \, \tilde{g}_t^{(l)}.
$$

---

## 2. Моментум только в последнем слое

**Гипотеза авторов статьи**: дисперсия стохастических градиентов неравномерна по слоям — она максимальна в выходном слое. Применение момента ко всем слоям избыточно и затратно по памяти. Достаточно сгладить шум только в выходном слое

**Реализация**: вводится вспомогательная переменная $m_t^{(L)}$ только для последнего слоя $l = L$:

$$
m_t^{(L)} = \beta \, m_{t-1}^{(L)} + (1 - \beta) \, \tilde{g}_t^{(L)},
$$

а обновление:

$$
\theta_{t+1}^{(L)} = \theta_t^{(L)} - \eta \, m_t^{(L)}.
$$

Для всех остальных слоёв $l \ne L$:

$$
\theta_{t+1}^{(l)} = \theta_t^{(l)} - \eta \, \tilde{g}_t^{(l)}.
$$

---

### 3. Полный алгоритм SCALE

Соображение: объединение столбцовой нормализации (для стабилизации масштаба) и последнего слоя с моментумом (для снижения дисперсии) даёт простой, но эффективный оптимизатор, не требующий хранения полных моментов для всех параметров.

(тут возникли сложности у рендеринга формул в гитхабе, поэтому прикладываю скриншот оформления с Google Colab)
<img width="1243" height="495" alt="image" src="https://github.com/user-attachments/assets/8255fa00-f6c3-41d4-a231-6d89afaf9d84" />



# Почему моментум не дал значительного прироста в данной лабораторной работе

В данной лабораторной работе при эксперементе с автоэнкодером на Fashion-MNIST основной прирост качества дала **столбцовая нормализация градиентов**, тогда как добавление **момента только к последнему слою** почти не повлияло на результаты.

Это может быть логично, так как авторы статьи утверждают, что моментум нужен для стабилизации наиболее шумных градиентов. Они ставили эксперименты на больших языковых моделях - сложных нейросетях, в то время, как в работе обучалась довольно маленькая нейросеть (784 → 64 → 32 → 64 → 784). Наиболее вероятно, что у неё во время обучения не наблюдалась высокая дисперсия градиентов ни на одном из слоёв. Более того, моментум наиболее эффективно работает при большом числе эпох, в то время как для решения задачи оказалось достаточно всего лишь 20. Это косвенно подтверждает наблюдение статьи: моментум эффективен **только в слоях с высокой дисперсией градиентов**.
